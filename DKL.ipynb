{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Nanophotonics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomsx = torch.Tensor(loadmat('FOMs_x.mat', mat_dtype=True)['RVs_rnd'])\n",
    "fomsy = torch.Tensor(loadmat('FOMs_y.mat', mat_dtype=True)['FOMs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F0 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = torch.cat((fomsx[:,0:5], fomsy[:,0:1]), 1)\n",
    "f0 = f0[torch.randperm(f0.size()[0])]\n",
    "data = f0\n",
    "#if not os.path.isfile('elevators.mat'):\n",
    "#    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "#    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', 'elevators.mat')\n",
    "\n",
    "#data = torch.Tensor(loadmat('elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 316070.625\n",
      "Iter 2/60 - Loss: 309448.188\n",
      "Iter 3/60 - Loss: 308253.219\n",
      "Iter 4/60 - Loss: 303485.375\n",
      "Iter 5/60 - Loss: 299088.781\n",
      "Iter 6/60 - Loss: 295577.312\n",
      "Iter 7/60 - Loss: 292145.844\n",
      "Iter 8/60 - Loss: 288821.594\n",
      "Iter 9/60 - Loss: 286085.781\n",
      "Iter 10/60 - Loss: 283105.406\n",
      "Iter 11/60 - Loss: 279701.938\n",
      "Iter 12/60 - Loss: 276345.469\n",
      "Iter 13/60 - Loss: 273538.188\n",
      "Iter 14/60 - Loss: 270164.844\n",
      "Iter 15/60 - Loss: 266928.844\n",
      "Iter 16/60 - Loss: 263319.469\n",
      "Iter 17/60 - Loss: 260412.109\n",
      "Iter 18/60 - Loss: 257992.734\n",
      "Iter 19/60 - Loss: 254647.266\n",
      "Iter 20/60 - Loss: 251479.234\n",
      "Iter 21/60 - Loss: 248186.828\n",
      "Iter 22/60 - Loss: 247364.766\n",
      "Iter 23/60 - Loss: 250328.406\n",
      "Iter 24/60 - Loss: 248557.516\n",
      "Iter 25/60 - Loss: 245838.516\n",
      "Iter 26/60 - Loss: 242942.141\n",
      "Iter 27/60 - Loss: 238439.234\n",
      "Iter 28/60 - Loss: 235690.781\n",
      "Iter 29/60 - Loss: 232020.172\n",
      "Iter 30/60 - Loss: 228226.219\n",
      "Iter 31/60 - Loss: 225700.219\n",
      "Iter 32/60 - Loss: 222669.406\n",
      "Iter 33/60 - Loss: 220501.422\n",
      "Iter 34/60 - Loss: 218239.938\n",
      "Iter 35/60 - Loss: 215166.641\n",
      "Iter 36/60 - Loss: 214831.391\n",
      "Iter 37/60 - Loss: 210784.156\n",
      "Iter 38/60 - Loss: 206637.703\n",
      "Iter 39/60 - Loss: 205103.094\n",
      "Iter 40/60 - Loss: 204473.000\n",
      "Iter 41/60 - Loss: 198610.922\n",
      "Iter 42/60 - Loss: 197747.938\n",
      "Iter 43/60 - Loss: 201636.812\n",
      "Iter 44/60 - Loss: 200885.438\n",
      "Iter 45/60 - Loss: 203277.422\n",
      "Iter 46/60 - Loss: 195368.531\n",
      "Iter 47/60 - Loss: 196329.344\n",
      "Iter 48/60 - Loss: 191140.328\n",
      "Iter 49/60 - Loss: 189039.906\n",
      "Iter 50/60 - Loss: 185390.203\n",
      "Iter 51/60 - Loss: 185105.984\n",
      "Iter 52/60 - Loss: 187292.297\n",
      "Iter 53/60 - Loss: 179829.859\n",
      "Iter 54/60 - Loss: 178866.828\n",
      "Iter 55/60 - Loss: 174057.438\n",
      "Iter 56/60 - Loss: 170771.594\n",
      "Iter 57/60 - Loss: 170368.859\n",
      "Iter 58/60 - Loss: 168962.688\n",
      "Iter 59/60 - Loss: 162979.641\n",
      "Iter 60/60 - Loss: 167712.719\n",
      "CPU times: user 10min 6s, sys: 4.97 s, total: 10min 11s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 426.04217529296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:247: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    train_preds = model(train_x)\n",
    "print('Train MAE: {}'.format(torch.mean(torch.abs(train_preds.mean - train_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 534.7948608398438\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.cat((fomsx[:,0:5], fomsy[:,1:2]), 1)\n",
    "f1 = f1[torch.randperm(f0.size()[0])]\n",
    "data = f1\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 78329.734\n",
      "Iter 2/60 - Loss: 75709.570\n",
      "Iter 3/60 - Loss: 71425.562\n",
      "Iter 4/60 - Loss: 69685.180\n",
      "Iter 5/60 - Loss: 67653.703\n",
      "Iter 6/60 - Loss: 66153.445\n",
      "Iter 7/60 - Loss: 64489.352\n",
      "Iter 8/60 - Loss: 62887.633\n",
      "Iter 9/60 - Loss: 60827.512\n",
      "Iter 10/60 - Loss: 59943.648\n",
      "Iter 11/60 - Loss: 58588.652\n",
      "Iter 12/60 - Loss: 57837.777\n",
      "Iter 13/60 - Loss: 55990.848\n",
      "Iter 14/60 - Loss: 55376.008\n",
      "Iter 15/60 - Loss: 54486.082\n",
      "Iter 16/60 - Loss: 52747.555\n",
      "Iter 17/60 - Loss: 51616.535\n",
      "Iter 18/60 - Loss: 50745.480\n",
      "Iter 19/60 - Loss: 49326.977\n",
      "Iter 20/60 - Loss: 48739.941\n",
      "Iter 21/60 - Loss: 47984.977\n",
      "Iter 22/60 - Loss: 46625.273\n",
      "Iter 23/60 - Loss: 45830.047\n",
      "Iter 24/60 - Loss: 45367.789\n",
      "Iter 25/60 - Loss: 45465.441\n",
      "Iter 26/60 - Loss: 43923.406\n",
      "Iter 27/60 - Loss: 43364.781\n",
      "Iter 28/60 - Loss: 42830.148\n",
      "Iter 29/60 - Loss: 41213.836\n",
      "Iter 30/60 - Loss: 40647.664\n",
      "Iter 31/60 - Loss: 40651.199\n",
      "Iter 32/60 - Loss: 40183.098\n",
      "Iter 33/60 - Loss: 40771.688\n",
      "Iter 34/60 - Loss: 39745.715\n",
      "Iter 35/60 - Loss: 39858.695\n",
      "Iter 36/60 - Loss: 39145.352\n",
      "Iter 37/60 - Loss: 37649.496\n",
      "Iter 38/60 - Loss: 37552.227\n",
      "Iter 39/60 - Loss: 36788.664\n",
      "Iter 40/60 - Loss: 35911.059\n",
      "Iter 41/60 - Loss: 35208.230\n",
      "Iter 42/60 - Loss: 36342.887\n",
      "Iter 43/60 - Loss: 34553.723\n",
      "Iter 44/60 - Loss: 35913.188\n",
      "Iter 45/60 - Loss: 34607.828\n",
      "Iter 46/60 - Loss: 33984.496\n",
      "Iter 47/60 - Loss: 34229.812\n",
      "Iter 48/60 - Loss: 32320.033\n",
      "Iter 49/60 - Loss: 31900.975\n",
      "Iter 50/60 - Loss: 30898.695\n",
      "Iter 51/60 - Loss: 30991.502\n",
      "Iter 52/60 - Loss: 30384.252\n",
      "Iter 53/60 - Loss: 33972.742\n",
      "Iter 54/60 - Loss: 32622.428\n",
      "Iter 55/60 - Loss: 31640.092\n",
      "Iter 56/60 - Loss: 31429.320\n",
      "Iter 57/60 - Loss: 30506.848\n",
      "Iter 58/60 - Loss: 30729.477\n",
      "Iter 59/60 - Loss: 28880.879\n",
      "Iter 60/60 - Loss: 29676.658\n",
      "CPU times: user 10min 8s, sys: 5.74 s, total: 10min 14s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 151.2612762451172\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4983, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = torch.cat((fomsx[:,0:5], fomsy[:,2:3]), 1)\n",
    "f2 = f2[torch.randperm(f0.size()[0])]\n",
    "data = f2\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 140728.109\n",
      "Iter 2/60 - Loss: 135678.375\n",
      "Iter 3/60 - Loss: 128216.062\n",
      "Iter 4/60 - Loss: 125743.219\n",
      "Iter 5/60 - Loss: 120583.156\n",
      "Iter 6/60 - Loss: 118685.500\n",
      "Iter 7/60 - Loss: 115023.516\n",
      "Iter 8/60 - Loss: 111105.977\n",
      "Iter 9/60 - Loss: 107162.234\n",
      "Iter 10/60 - Loss: 103565.539\n",
      "Iter 11/60 - Loss: 100971.406\n",
      "Iter 12/60 - Loss: 99462.922\n",
      "Iter 13/60 - Loss: 97114.367\n",
      "Iter 14/60 - Loss: 94522.398\n",
      "Iter 15/60 - Loss: 92956.523\n",
      "Iter 16/60 - Loss: 91233.789\n",
      "Iter 17/60 - Loss: 88963.742\n",
      "Iter 18/60 - Loss: 87190.609\n",
      "Iter 19/60 - Loss: 85849.875\n",
      "Iter 20/60 - Loss: 84115.234\n",
      "Iter 21/60 - Loss: 82339.727\n",
      "Iter 22/60 - Loss: 81023.695\n",
      "Iter 23/60 - Loss: 79612.508\n",
      "Iter 24/60 - Loss: 78161.945\n",
      "Iter 25/60 - Loss: 76797.984\n",
      "Iter 26/60 - Loss: 75512.547\n",
      "Iter 27/60 - Loss: 73915.328\n",
      "Iter 28/60 - Loss: 72722.336\n",
      "Iter 29/60 - Loss: 72049.648\n",
      "Iter 30/60 - Loss: 72479.477\n",
      "Iter 31/60 - Loss: 72830.672\n",
      "Iter 32/60 - Loss: 72102.078\n",
      "Iter 33/60 - Loss: 70202.242\n",
      "Iter 34/60 - Loss: 68431.594\n",
      "Iter 35/60 - Loss: 67884.445\n",
      "Iter 36/60 - Loss: 66686.789\n",
      "Iter 37/60 - Loss: 65366.188\n",
      "Iter 38/60 - Loss: 65192.914\n",
      "Iter 39/60 - Loss: 63029.398\n",
      "Iter 40/60 - Loss: 62781.074\n",
      "Iter 41/60 - Loss: 60844.609\n",
      "Iter 42/60 - Loss: 59856.879\n",
      "Iter 43/60 - Loss: 58196.293\n",
      "Iter 44/60 - Loss: 57167.395\n",
      "Iter 45/60 - Loss: 57629.391\n",
      "Iter 46/60 - Loss: 58846.375\n",
      "Iter 47/60 - Loss: 60364.836\n",
      "Iter 48/60 - Loss: 59726.180\n",
      "Iter 49/60 - Loss: 65437.609\n",
      "Iter 50/60 - Loss: 60426.199\n",
      "Iter 51/60 - Loss: 58462.289\n",
      "Iter 52/60 - Loss: 59335.609\n",
      "Iter 53/60 - Loss: 57026.777\n",
      "Iter 54/60 - Loss: 56801.020\n",
      "Iter 55/60 - Loss: 53557.344\n",
      "Iter 56/60 - Loss: 54993.430\n",
      "Iter 57/60 - Loss: 51756.773\n",
      "Iter 58/60 - Loss: 51647.512\n",
      "Iter 59/60 - Loss: 49915.496\n",
      "Iter 60/60 - Loss: 49924.500\n",
      "CPU times: user 10min 10s, sys: 6.1 s, total: 10min 16s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 223.6408233642578\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = torch.cat((fomsx[:,0:5], fomsy[:,2:3]), 1)\n",
    "f2 = f2[torch.randperm(f0.size()[0])]\n",
    "data = f2\n",
    "\n",
    "X_nano = data[:, :-1]\n",
    "X_nano = X_nano - X_nano.min(0)[0]\n",
    "X_nano = 2 * (X_nano / X_nano.max(0)[0]) - 1\n",
    "y_nano = data[:, -1]\n",
    "y_nano = y_nano - y_nano.min(0)[0]\n",
    "y_nano = 2 * (y_nano / y_nano.max(0)[0]) - 1\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X_nano[:train_n, :].contiguous()\n",
    "train_y = y_nano[:train_n].contiguous()\n",
    "\n",
    "test_x = X_nano[train_n:, :].contiguous()\n",
    "test_y = y_nano[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4983, 5]),\n",
       " torch.Size([4983]),\n",
       " torch.Size([3986, 5]),\n",
       " torch.Size([3986]),\n",
       " torch.Size([997, 5]),\n",
       " torch.Size([997]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size(), y.size(), train_x.size(), train_y.size(), test_x.size(), test_y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with regular GP with squared exponential kernel (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nanophotonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3986, 5]),\n",
       " torch.Size([3986]),\n",
       " torch.Size([100, 2]),\n",
       " torch.Size([997]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.size(), train_y.size(), test_x.size(), test_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # SKI requires a grid size hyperparameter. This util can help with that\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(ard_num_dims=5),\n",
    "            ), grid_size=grid_size, num_dims=5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/30 - Loss: 0.854\n",
      "Iter 2/30 - Loss: 0.826\n",
      "Iter 3/30 - Loss: 0.797\n",
      "Iter 4/30 - Loss: 0.770\n",
      "Iter 5/30 - Loss: 0.743\n",
      "Iter 6/30 - Loss: 0.715\n",
      "Iter 7/30 - Loss: 0.689\n",
      "Iter 8/30 - Loss: 0.662\n",
      "Iter 9/30 - Loss: 0.637\n",
      "Iter 10/30 - Loss: 0.613\n",
      "Iter 11/30 - Loss: 0.590\n",
      "Iter 12/30 - Loss: 0.569\n",
      "Iter 13/30 - Loss: 0.547\n",
      "Iter 14/30 - Loss: 0.530\n",
      "Iter 15/30 - Loss: 0.513\n",
      "Iter 16/30 - Loss: 0.499\n",
      "Iter 17/30 - Loss: 0.492\n",
      "Iter 18/30 - Loss: 0.479\n",
      "Iter 19/30 - Loss: 0.476\n",
      "Iter 20/30 - Loss: 0.470\n",
      "Iter 21/30 - Loss: 0.469\n",
      "Iter 22/30 - Loss: 0.464\n",
      "Iter 23/30 - Loss: 0.469\n",
      "Iter 24/30 - Loss: 0.471\n",
      "Iter 25/30 - Loss: 0.472\n",
      "Iter 26/30 - Loss: 0.477\n",
      "Iter 27/30 - Loss: 0.484\n",
      "Iter 28/30 - Loss: 0.484\n",
      "Iter 29/30 - Loss: 0.485\n",
      "Iter 30/30 - Loss: 0.482\n",
      "CPU times: user 40min 8s, sys: 27 s, total: 40min 35s\n",
      "Wall time: 10min 52s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "def train():\n",
    "    training_iterations = 30\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds_train = model(train_x)\n",
    "    \n",
    "print('Train MAE: {}'.format(torch.mean(torch.abs(preds_train.mean - train_y))))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)\n",
    "    \n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matern Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # SKI requires a grid size hyperparameter. This util can help with that\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(ard_num_dims=5),\n",
    "            ), grid_size=grid_size, num_dims=5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_mat = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model_mat.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model_mat.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model_mat)\n",
    "\n",
    "def train():\n",
    "    training_iterations = 50\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model_mat(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mat.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds_train = model_mat(train_x)\n",
    "    \n",
    "print('Train MAE: {}'.format(torch.mean(torch.abs(preds_train.mean - train_y))))\n",
    "\n",
    "model_mat.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds = model_mat(test_x)\n",
    "    \n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astronomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrateMethod = integrate.trapz\n",
    "#from snls.SNMFOptFunction import snlsLogLikl\n",
    "def snlsLogLikl(evalPts, obsData, resolution, numObsToUse):\n",
    "  \"\"\" This computes the log likelihood. \"\"\"\n",
    "\n",
    "  # Round them before proceeding\n",
    "  resolution = int(resolution)\n",
    "  numObsToUse = int(numObsToUse)\n",
    "\n",
    "  # Define some constants\n",
    "  LIGHT_SPEED = 299792.48\n",
    "\n",
    "  # Prelims\n",
    "  numObs = obsData.shape[0]\n",
    "  numPts = evalPts.shape[0]\n",
    "  if numObsToUse is None:\n",
    "    numObsToUse = numObs\n",
    "\n",
    "  # Decompose Data\n",
    "  useObsData = obsData[0:numObsToUse, :]\n",
    "  z = useObsData[:, [0]]\n",
    "  obs = useObsData[:, [1]]\n",
    "  obsErr = useObsData[:, [2]]\n",
    "\n",
    "  # Create arrays for storing outputs\n",
    "  logJointProbs = np.zeros((numPts, 1))\n",
    "  lumMeans = np.zeros((numPts, numObsToUse))\n",
    "\n",
    "  # Now iterate through each of the evalPts\n",
    "  for i in range(numPts):\n",
    "    H = evalPts[i, 0]\n",
    "    OmegaM = evalPts[i, 1]\n",
    "    OmegaL = evalPts[i, 2]\n",
    "    currLumMeans = np.zeros((numObsToUse, 1))\n",
    "\n",
    "    f = lambda t: 1/ np.sqrt(OmegaM * (1+t)**3 + OmegaL)\n",
    "    for obsIter in range(numObsToUse):\n",
    "      dLC = LIGHT_SPEED * (1 + z[obsIter, 0])/H\n",
    "      integrate_grid = np.linspace(0, z[obsIter, 0], resolution)\n",
    "      integrate_grid_res = integrate_grid[1] - integrate_grid[0]\n",
    "      integrate_values = f(integrate_grid)\n",
    "      dLI = integrateMethod(integrate_values, dx=integrate_grid_res)\n",
    "      dL = dLC * dLI\n",
    "      currLumMeans[obsIter, 0] = 5 * np.log10(dL) + 25\n",
    "    obsLikl = norm.pdf(obs, currLumMeans, obsErr)\n",
    "    obsLogLikl = sum(np.log(obsLikl))\n",
    "\n",
    "    obsLogLikl = obsLogLikl\n",
    "    logJointProbs[i, 0] = obsLogLikl\n",
    "    logJointProbs[i, 0] = logJointProbs[i, 0] / numObsToUse\n",
    "    lumMeans[[i], :] = currLumMeans.transpose()\n",
    "\n",
    "  return logJointProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[78.50262781,  0.48920912,  0.94813763],\n",
       "        [63.79143157,  0.14882729,  0.25895187],\n",
       "        [70.04099213,  0.60479575,  0.48826584],\n",
       "        ...,\n",
       "        [63.15587325,  0.87843793,  0.68956331],\n",
       "        [76.61496609,  0.58598371,  0.43431912],\n",
       "        [65.20416802,  0.24215812,  0.61851402]]), array([[ -7.76943453],\n",
       "        [-10.67393402],\n",
       "        [ -1.8549358 ],\n",
       "        ...,\n",
       "        [ -3.72461854],\n",
       "        [ -3.23301544],\n",
       "        [ -0.23571951]]))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10000\n",
    "obsData = np.loadtxt('davisdata.txt')\n",
    "x1 = np.random.uniform(60,80,(N,))\n",
    "x2 = np.random.uniform(0.1,1,(N,))\n",
    "x3 = np.random.uniform(0.1,1,(N,))\n",
    "x = np.transpose(np.array([x1,x2,x3]))\n",
    "y = snlsLogLikl(x, obsData, resolution=N, numObsToUse=192)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[78.50262781,  0.48920912,  0.94813763],\n",
       "        [63.79143157,  0.14882729,  0.25895187],\n",
       "        [70.04099213,  0.60479575,  0.48826584],\n",
       "        ...,\n",
       "        [63.15587325,  0.87843793,  0.68956331],\n",
       "        [76.61496609,  0.58598371,  0.43431912],\n",
       "        [65.20416802,  0.24215812,  0.61851402]]), array([[ -7.76943453],\n",
       "        [-10.67393402],\n",
       "        [ -1.8549358 ],\n",
       "        ...,\n",
       "        [ -3.72461854],\n",
       "        [ -3.23301544],\n",
       "        [ -0.23571951]]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_astro = x\n",
    "X_astro = X_astro - X_astro.min(0)[0]\n",
    "X_astro = 2 * (X_astro / X_astro.max(0)[0]) - 1\n",
    "y_astro = y\n",
    "y_astro = y_astro - y_astro.min(0)[0]\n",
    "y_astro = 2 * (y_astro / y_astro.max(0)[0]) - 1\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X_nano[:train_n, :].contiguous()\n",
    "train_y = y_nano[:train_n].contiguous()\n",
    "\n",
    "test_x = X_nano[train_n:, :].contiguous()\n",
    "test_y = y_nano[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.size(), train_y.size(), test_x.size(), test_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # SKI requires a grid size hyperparameter. This util can help with that\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.RBFKernel(ard_num_dims=5),\n",
    "            ), grid_size=grid_size, num_dims=5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 0.854\n",
      "Iter 2/200 - Loss: 0.826\n",
      "Iter 3/200 - Loss: 0.797\n",
      "Iter 4/200 - Loss: 0.769\n",
      "Iter 5/200 - Loss: 0.741\n",
      "Iter 6/200 - Loss: 0.715\n",
      "Iter 7/200 - Loss: 0.689\n",
      "Iter 8/200 - Loss: 0.662\n",
      "Iter 9/200 - Loss: 0.636\n",
      "Iter 10/200 - Loss: 0.614\n",
      "Iter 11/200 - Loss: 0.590\n",
      "Iter 12/200 - Loss: 0.568\n",
      "Iter 13/200 - Loss: 0.550\n",
      "Iter 14/200 - Loss: 0.530\n",
      "Iter 15/200 - Loss: 0.516\n",
      "Iter 16/200 - Loss: 0.501\n",
      "Iter 17/200 - Loss: 0.491\n",
      "Iter 18/200 - Loss: 0.481\n",
      "Iter 19/200 - Loss: 0.473\n",
      "Iter 20/200 - Loss: 0.473\n",
      "Iter 21/200 - Loss: 0.468\n",
      "Iter 22/200 - Loss: 0.469\n",
      "Iter 23/200 - Loss: 0.469\n",
      "Iter 24/200 - Loss: 0.474\n",
      "Iter 25/200 - Loss: 0.478\n",
      "Iter 26/200 - Loss: 0.473\n",
      "Iter 27/200 - Loss: 0.486\n",
      "Iter 28/200 - Loss: 0.484\n",
      "Iter 29/200 - Loss: 0.487\n",
      "Iter 30/200 - Loss: 0.479\n",
      "Iter 31/200 - Loss: 0.489\n",
      "Iter 32/200 - Loss: 0.493\n",
      "Iter 33/200 - Loss: 0.493\n",
      "Iter 34/200 - Loss: 0.479\n",
      "Iter 35/200 - Loss: 0.494\n",
      "Iter 36/200 - Loss: 0.480\n",
      "Iter 37/200 - Loss: 0.508\n",
      "Iter 38/200 - Loss: 0.481\n",
      "Iter 39/200 - Loss: 0.518\n",
      "Iter 40/200 - Loss: 0.527\n",
      "Iter 41/200 - Loss: 0.538\n",
      "Iter 42/200 - Loss: 0.548\n",
      "Iter 43/200 - Loss: 0.567\n",
      "Iter 44/200 - Loss: 0.552\n",
      "Iter 45/200 - Loss: 0.586\n",
      "Iter 46/200 - Loss: 0.572\n",
      "Iter 47/200 - Loss: 0.583\n",
      "Iter 48/200 - Loss: 0.616\n",
      "Iter 49/200 - Loss: 0.643\n",
      "Iter 50/200 - Loss: 0.644\n",
      "Iter 51/200 - Loss: 0.618\n",
      "Iter 52/200 - Loss: 0.701\n",
      "Iter 53/200 - Loss: 0.726\n",
      "Iter 54/200 - Loss: 0.733\n",
      "Iter 55/200 - Loss: 0.765\n",
      "Iter 56/200 - Loss: 0.784\n",
      "Iter 57/200 - Loss: 0.803\n",
      "Iter 58/200 - Loss: 0.839\n",
      "Iter 59/200 - Loss: 0.805\n",
      "Iter 60/200 - Loss: 0.822\n",
      "Iter 61/200 - Loss: 0.868\n",
      "Iter 62/200 - Loss: 0.893\n",
      "Iter 63/200 - Loss: 0.905\n",
      "Iter 64/200 - Loss: 0.897\n",
      "Iter 65/200 - Loss: 0.907\n",
      "Iter 66/200 - Loss: 0.897\n",
      "Iter 67/200 - Loss: 0.914\n",
      "Iter 68/200 - Loss: 0.885\n",
      "Iter 69/200 - Loss: 0.969\n",
      "Iter 70/200 - Loss: 0.976\n",
      "Iter 71/200 - Loss: 0.962\n",
      "Iter 72/200 - Loss: 0.886\n",
      "Iter 73/200 - Loss: 0.972\n",
      "Iter 74/200 - Loss: 0.985\n",
      "Iter 75/200 - Loss: 0.940\n",
      "Iter 76/200 - Loss: 0.955\n",
      "Iter 77/200 - Loss: 0.943\n",
      "Iter 78/200 - Loss: 0.932\n",
      "Iter 79/200 - Loss: 0.899\n",
      "Iter 80/200 - Loss: 0.933\n",
      "Iter 81/200 - Loss: 0.904\n",
      "Iter 82/200 - Loss: 0.951\n",
      "Iter 83/200 - Loss: 0.947\n",
      "Iter 84/200 - Loss: 0.918\n",
      "Iter 85/200 - Loss: 0.915\n",
      "Iter 86/200 - Loss: 0.922\n",
      "Iter 87/200 - Loss: 0.991\n",
      "Iter 88/200 - Loss: 0.940\n",
      "Iter 89/200 - Loss: 0.918\n",
      "Iter 90/200 - Loss: 0.930\n",
      "Iter 91/200 - Loss: 0.971\n",
      "Iter 92/200 - Loss: 0.961\n",
      "Iter 93/200 - Loss: 0.933\n",
      "Iter 94/200 - Loss: 0.922\n",
      "Iter 95/200 - Loss: 0.918\n",
      "Iter 96/200 - Loss: 0.930\n",
      "Iter 97/200 - Loss: 0.975\n",
      "Iter 98/200 - Loss: 0.936\n",
      "Iter 99/200 - Loss: 0.957\n",
      "Iter 100/200 - Loss: 0.916\n",
      "Iter 101/200 - Loss: 0.925\n",
      "Iter 102/200 - Loss: 0.904\n",
      "Iter 103/200 - Loss: 0.948\n",
      "Iter 104/200 - Loss: 0.924\n",
      "Iter 105/200 - Loss: 0.939\n",
      "Iter 106/200 - Loss: 0.901\n",
      "Iter 107/200 - Loss: 0.886\n",
      "Iter 108/200 - Loss: 0.921\n",
      "Iter 109/200 - Loss: 0.847\n",
      "Iter 110/200 - Loss: 0.899\n",
      "Iter 111/200 - Loss: 0.896\n",
      "Iter 112/200 - Loss: 0.902\n",
      "Iter 113/200 - Loss: 0.898\n",
      "Iter 114/200 - Loss: 0.925\n",
      "Iter 115/200 - Loss: 0.917\n",
      "Iter 116/200 - Loss: 0.866\n",
      "Iter 117/200 - Loss: 0.905\n",
      "Iter 118/200 - Loss: 0.960\n",
      "Iter 119/200 - Loss: 0.897\n",
      "Iter 120/200 - Loss: 0.887\n",
      "Iter 121/200 - Loss: 0.923\n",
      "Iter 122/200 - Loss: 0.912\n",
      "Iter 123/200 - Loss: 0.945\n",
      "Iter 124/200 - Loss: 0.929\n",
      "Iter 125/200 - Loss: 0.930\n",
      "Iter 126/200 - Loss: 0.915\n",
      "Iter 127/200 - Loss: 0.930\n",
      "Iter 128/200 - Loss: 0.925\n",
      "Iter 129/200 - Loss: 0.876\n",
      "Iter 130/200 - Loss: 0.872\n",
      "Iter 131/200 - Loss: 0.925\n",
      "Iter 132/200 - Loss: 0.908\n",
      "Iter 133/200 - Loss: 0.871\n",
      "Iter 134/200 - Loss: 0.917\n",
      "Iter 135/200 - Loss: 0.892\n",
      "Iter 136/200 - Loss: 0.909\n",
      "Iter 137/200 - Loss: 0.896\n",
      "Iter 138/200 - Loss: 0.922\n",
      "Iter 139/200 - Loss: 0.911\n",
      "Iter 140/200 - Loss: 0.923\n",
      "Iter 141/200 - Loss: 0.913\n",
      "Iter 142/200 - Loss: 0.927\n",
      "Iter 143/200 - Loss: 0.890\n",
      "Iter 144/200 - Loss: 0.924\n",
      "Iter 145/200 - Loss: 0.931\n",
      "Iter 146/200 - Loss: 0.916\n",
      "Iter 147/200 - Loss: 0.895\n",
      "Iter 148/200 - Loss: 0.896\n",
      "Iter 149/200 - Loss: 0.902\n",
      "Iter 150/200 - Loss: 0.952\n",
      "Iter 151/200 - Loss: 0.903\n",
      "Iter 152/200 - Loss: 0.932\n",
      "Iter 153/200 - Loss: 0.893\n",
      "Iter 154/200 - Loss: 0.919\n",
      "Iter 155/200 - Loss: 0.867\n",
      "Iter 156/200 - Loss: 0.853\n",
      "Iter 157/200 - Loss: 0.864\n",
      "Iter 158/200 - Loss: 0.938\n",
      "Iter 159/200 - Loss: 0.936\n",
      "Iter 160/200 - Loss: 0.962\n",
      "Iter 161/200 - Loss: 0.990\n",
      "Iter 162/200 - Loss: 0.923\n",
      "Iter 163/200 - Loss: 0.955\n",
      "Iter 164/200 - Loss: 0.988\n",
      "Iter 165/200 - Loss: 0.951\n",
      "Iter 166/200 - Loss: 1.009\n",
      "Iter 167/200 - Loss: 1.005\n",
      "Iter 168/200 - Loss: 0.996\n",
      "Iter 169/200 - Loss: 0.968\n",
      "Iter 170/200 - Loss: 1.025\n",
      "Iter 171/200 - Loss: 1.003\n",
      "Iter 172/200 - Loss: 1.001\n",
      "Iter 173/200 - Loss: 0.995\n",
      "Iter 174/200 - Loss: 1.018\n",
      "Iter 175/200 - Loss: 0.988\n",
      "Iter 176/200 - Loss: 0.986\n",
      "Iter 177/200 - Loss: 0.984\n",
      "Iter 178/200 - Loss: 0.975\n",
      "Iter 179/200 - Loss: 0.986\n",
      "Iter 180/200 - Loss: 1.002\n",
      "Iter 181/200 - Loss: 1.011\n",
      "Iter 182/200 - Loss: 0.967\n",
      "Iter 183/200 - Loss: 0.958\n",
      "Iter 184/200 - Loss: 0.987\n",
      "Iter 185/200 - Loss: 1.044\n",
      "Iter 186/200 - Loss: 0.961\n",
      "Iter 187/200 - Loss: 0.973\n",
      "Iter 188/200 - Loss: 0.935\n",
      "Iter 189/200 - Loss: 0.932\n",
      "Iter 190/200 - Loss: 0.932\n",
      "Iter 191/200 - Loss: 0.936\n",
      "Iter 192/200 - Loss: 0.954\n",
      "Iter 193/200 - Loss: 0.924\n",
      "Iter 194/200 - Loss: 0.922\n",
      "Iter 195/200 - Loss: 0.922\n",
      "Iter 196/200 - Loss: 0.938\n",
      "Iter 197/200 - Loss: 0.933\n",
      "Iter 198/200 - Loss: 0.956\n",
      "Iter 199/200 - Loss: 0.951\n",
      "Iter 200/200 - Loss: 0.947\n",
      "CPU times: user 23h 31min 18s, sys: 13min 35s, total: 23h 44min 53s\n",
      "Wall time: 4h 40min 58s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "def train():\n",
    "    training_iterations = 50\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds_train = model(train_x)\n",
    "    \n",
    "print('Train MAE: {}'.format(torch.mean(torch.abs(preds_train.mean - train_y))))\n",
    "\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)\n",
    "    \n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000, -0.6000,  0.8000, -0.3333,  1.0000],\n",
       "         [-0.1765, -0.8000,  0.6000,  0.6000,  0.4000],\n",
       "         [ 0.2941,  0.8000,  0.6000,  0.6000,  0.2000],\n",
       "         ...,\n",
       "         [-0.7647,  0.8000, -0.5000,  0.4667,  0.6000],\n",
       "         [-0.0588,  0.8000,  0.0000, -0.3333,  1.0000],\n",
       "         [-0.5294, -0.6000, -0.4000,  1.0000, -0.4000]]),\n",
       " tensor([[ 1.0000,  1.0000,  0.5000, -0.3333,  0.2000],\n",
       "         [-0.0588, -0.4000, -0.2000, -1.0000,  0.2000],\n",
       "         [-0.1765,  0.4000, -0.1000, -0.2000,  1.0000],\n",
       "         ...,\n",
       "         [-0.7647, -1.0000, -0.4000, -0.8667,  0.6000],\n",
       "         [ 0.2941,  1.0000,  1.0000,  0.6000, -0.4000],\n",
       "         [-0.6471,  0.6000,  0.0000,  0.3333,  0.0000]]),\n",
       " tensor([-0.6831, -0.5761, -0.5747,  ..., -0.5244, -0.7934, -0.8485]),\n",
       " tensor([-0.7902, -0.6887, -0.7329, -0.5410, -0.6914, -0.6122, -0.4607, -0.7129,\n",
       "         -0.7670, -0.8458, -0.5351, -0.8709, -0.6627, -0.8548, -0.8672,  0.5121,\n",
       "         -0.6566,  0.3160, -0.5680, -0.7795, -0.6920,  0.5972,  0.4308, -0.6932,\n",
       "         -0.7327, -0.7258, -0.6101, -0.7633, -0.6526, -0.8189, -0.5547, -0.5877,\n",
       "          0.4910,  0.6941, -0.6385, -0.7950, -0.7274, -0.9384, -0.3334, -0.6157,\n",
       "          0.3294,  0.5810, -0.8345, -0.8949, -0.5083, -0.5239, -0.6791, -0.7726,\n",
       "         -0.6326, -0.8769, -0.6840,  0.4599,  0.6126, -0.8754, -0.6189, -0.5275,\n",
       "         -0.8818, -0.8652, -0.6075, -0.6855, -0.8555,  0.7444,  0.9265, -0.7587,\n",
       "         -0.8894, -0.7998, -0.7735, -0.8200, -0.4182, -0.7376, -0.7034, -0.6123,\n",
       "         -0.2017, -0.5220, -0.5146, -0.4236,  0.6910, -0.8261, -0.7495, -0.6729,\n",
       "         -0.7290, -0.6262, -0.6255, -0.7177, -0.8232, -0.6273, -0.8193, -0.7765,\n",
       "          0.4457, -0.7974, -0.7601, -0.6947,  0.4949,  0.3445, -0.6124, -0.7272,\n",
       "         -0.7960,  0.4457, -0.6769, -0.5910, -0.4622, -0.6513, -0.7047, -0.6980,\n",
       "         -0.8941, -0.7213, -0.8843, -0.5955, -0.5493, -0.6283, -0.6502, -0.6492,\n",
       "         -0.9188, -0.6014,  0.4571, -0.8795,  0.4634, -0.6474, -0.7263, -0.7874,\n",
       "         -0.6626, -0.8376, -0.6441, -0.6472, -0.7230, -0.7822,  0.6977, -0.8289,\n",
       "         -0.7208, -0.7229, -0.4266, -0.6421, -0.7821, -0.5475, -0.8930, -0.7367,\n",
       "         -0.8115, -0.7301, -0.6417, -0.5982, -0.4787, -0.7642,  0.5909, -0.6461,\n",
       "          0.3675, -0.8123, -0.8438, -0.6968, -0.7684, -0.8763, -0.4725, -0.5734,\n",
       "         -0.9504, -0.7218,  0.5240, -0.5183, -0.8638, -0.8664, -0.6129, -0.8845,\n",
       "         -0.6305, -0.6921, -0.7357,  0.4562, -0.6415, -0.8227, -0.7965, -0.5834,\n",
       "         -0.7148, -0.5669,  0.7104,  0.5223, -0.8959, -0.8448, -0.7791, -0.5317,\n",
       "         -0.8212, -0.7222, -0.9078, -0.7798, -0.6296, -0.5065, -0.5074, -0.6461,\n",
       "         -0.7113,  0.4953, -0.7421, -0.7457,  0.8741,  0.5465, -0.7278, -0.3821,\n",
       "         -0.8802, -0.7720, -0.6705, -0.7836,  0.7471, -0.8071, -0.6889, -0.6833,\n",
       "         -0.7021, -0.7311,  1.0000,  0.4203, -0.5100, -0.6822, -0.8034, -0.5184,\n",
       "         -0.6658, -0.7542, -0.8637, -0.7746, -0.6443, -0.6198, -0.8589, -0.8971,\n",
       "         -0.7046, -0.7392, -0.7732, -0.5074, -0.6654, -0.6353, -0.6624, -0.5326,\n",
       "          0.5527, -0.5787, -0.6911, -0.7438, -0.5571, -0.4683, -0.8386, -0.7427,\n",
       "         -0.8855, -0.8650, -0.7372, -0.7660, -0.6580, -0.7768, -0.4835, -0.7631,\n",
       "         -0.5909, -0.5870, -0.8461, -0.6254, -0.4687, -0.6275, -0.7124, -0.6809,\n",
       "         -0.7080,  0.6116, -0.5415, -0.7816, -0.8070, -0.8053, -0.6799, -0.8315,\n",
       "         -0.5692, -0.7519, -0.9182, -0.7090, -0.7227, -0.7734, -0.7488, -0.7388,\n",
       "          0.4708, -0.6014, -0.7654, -0.3868, -0.6818, -0.6495,  0.4795, -0.8524,\n",
       "         -0.6362, -0.7978, -0.6374, -0.7325, -0.7972, -0.7920, -0.6242, -0.6478,\n",
       "          0.6082, -0.5818, -0.7690, -0.6488, -0.4432, -0.5172, -0.5648, -0.7190,\n",
       "         -0.7841, -0.8040, -0.7035, -0.6255, -0.5845, -0.9117,  0.5682, -0.6711,\n",
       "         -0.8922, -0.7042, -0.7163, -0.5346, -0.7229, -0.5050, -0.4670, -0.2567,\n",
       "         -0.4890, -0.8315, -0.6335, -0.6574, -0.8108, -0.7395,  0.4207, -0.8520,\n",
       "         -0.8430, -0.8661, -0.7956, -0.6302, -0.7740, -0.6206, -0.6302, -0.7649,\n",
       "         -0.5289, -0.4844, -0.8383, -0.8744, -0.8353, -0.6058,  0.5505, -0.8138,\n",
       "         -0.5006, -0.7667, -0.7639, -0.8838, -0.6902, -0.6147, -0.7189, -0.8415,\n",
       "          0.3574, -0.2984, -0.8440, -0.7745, -0.5368, -0.9447, -0.5522, -0.5909,\n",
       "         -0.6160,  0.6123, -0.4604, -0.7281, -0.4112, -0.7283, -0.7538, -0.8964,\n",
       "         -0.7215, -0.7052, -0.4325, -0.6471, -0.4434, -0.5841, -0.6863, -0.8946,\n",
       "         -0.7489, -0.6872, -0.6981, -0.7125, -0.8964, -0.8138, -0.7426, -0.5311,\n",
       "         -0.6015, -0.6230, -0.6583, -0.8434, -0.7936, -0.8346, -0.7293, -0.5711,\n",
       "         -0.8421, -0.7540, -0.7844, -0.7515, -0.7174, -0.6814, -0.8206, -0.7982,\n",
       "         -0.9658,  0.5262, -0.7046, -0.8284, -0.7399, -0.5791, -0.6773,  0.6455,\n",
       "         -0.6239, -0.8151, -0.6902, -0.7383,  0.4221, -0.7613, -0.7320, -0.7882,\n",
       "         -0.5587, -0.7994, -0.6299, -0.8662, -0.7751, -0.3924, -0.5459, -0.3441,\n",
       "         -0.7543, -0.8172, -0.8440, -0.8737, -0.4411, -0.7320, -0.8055, -0.7897,\n",
       "         -0.5672, -0.7801, -0.5001, -0.7966, -0.6914, -0.8295, -0.5821,  0.3774,\n",
       "         -0.8788,  0.5079, -0.2679, -0.7230,  0.4000, -0.5736,  0.5513,  0.0044,\n",
       "         -0.9170,  0.5335, -0.5022, -0.7623, -0.6852, -0.7217, -0.8826, -0.7079,\n",
       "         -0.7375, -0.6295, -0.6479, -0.7953, -0.7049, -0.8816, -0.5881, -0.8319,\n",
       "         -0.7792,  0.5411, -0.7009, -0.7766, -0.5803,  0.5743, -0.6871, -0.7612,\n",
       "         -0.8388, -0.7176, -0.6534, -0.6484, -0.8316,  0.5874, -0.3769, -0.6598,\n",
       "          0.5440, -0.7403, -0.8716, -0.7438, -0.6999, -0.6177, -0.7650, -0.7819,\n",
       "         -0.3746, -0.8436, -0.8340, -0.6282, -0.6392,  0.5994, -0.7305, -0.7683,\n",
       "         -0.3356, -0.7617, -0.7133, -0.4906, -0.8011, -0.5526, -0.8362, -0.6803,\n",
       "         -0.8968, -0.7547, -0.6462, -0.7916,  0.5068, -0.5439, -0.7205, -0.5730,\n",
       "         -0.4064, -0.5771, -0.6737, -0.8124,  0.3576,  0.5878, -0.5898, -0.7966,\n",
       "         -0.4919, -0.4597, -0.5794, -0.7701, -0.7764, -0.5721, -0.7694, -0.5510,\n",
       "         -0.6660, -0.6125, -0.8177, -0.6191, -0.5519, -0.6062, -0.8717,  0.5175,\n",
       "         -0.4123, -0.6607, -0.5752,  0.5494, -0.6580, -0.6205, -0.7909, -0.9036,\n",
       "         -0.7198, -0.6495, -0.7953, -0.7256, -0.7235, -0.6598, -0.5943, -0.8526,\n",
       "         -0.7098, -0.7542,  0.4529, -0.7623,  0.4613, -0.6414, -0.7819, -0.7607,\n",
       "         -0.2479, -0.4996, -0.7541, -0.6656, -0.5269, -0.8606, -0.4327, -0.8156,\n",
       "          0.7484, -0.6671, -0.7451, -0.8280, -0.7160, -0.8364, -0.6675, -0.8621,\n",
       "         -0.4129, -0.7719, -0.6480, -0.5666, -0.6999, -0.4968, -0.8479, -0.5344,\n",
       "         -0.5809, -0.3951,  0.6297, -0.6239, -0.7046,  0.4172, -0.6309, -0.6394,\n",
       "         -0.7679, -0.5771, -0.6702, -0.8051, -0.8442, -0.8582, -0.5527, -0.8039,\n",
       "          0.4183, -0.5395,  0.4197, -0.8846, -0.5081, -0.7775, -0.7590, -0.7481,\n",
       "         -0.5211, -0.5343, -0.8510, -0.6430, -0.9285, -0.6712, -0.7533, -0.4855,\n",
       "         -0.6884, -0.8094,  0.3669, -0.4371, -0.6477,  0.5485,  0.5881, -0.8818,\n",
       "         -0.4427, -0.7549,  0.4546, -0.7811, -0.8337,  0.5259, -0.8279, -0.5519,\n",
       "         -0.7462, -0.7209,  0.5117, -0.4636, -0.7669, -0.5284, -0.7335, -0.8752,\n",
       "         -0.6612, -0.8154, -0.7595, -0.7626, -0.5950, -0.8938, -0.6085, -0.5878,\n",
       "         -0.5032,  0.5389, -0.5914, -0.7702, -0.7034, -0.6336, -0.4873, -0.7404,\n",
       "          0.5467, -0.5537,  0.5184, -0.7923, -0.8847, -0.7533, -0.6174, -0.7265,\n",
       "         -0.6169, -0.6539,  0.5483, -0.7181, -0.8480, -0.6750, -0.4354, -0.5921,\n",
       "          0.3878, -0.4998, -0.7900, -0.7449, -0.5323, -0.6806, -0.5992, -0.7976,\n",
       "         -0.6058, -0.7326, -0.8437, -0.7955, -0.4133, -0.6686, -0.7467, -0.7387,\n",
       "         -0.6178, -0.5680, -0.8727, -0.8395, -0.4913, -0.8037, -0.8310,  0.6381,\n",
       "         -0.4176, -0.6372, -0.8418, -0.6342, -0.6689, -0.6383, -0.8490, -0.6101,\n",
       "         -0.8016, -0.6578, -0.6558, -0.6821,  0.5238, -0.6654, -0.6417, -0.5762,\n",
       "         -0.7878, -0.8650,  0.6666, -0.4639, -0.8215, -0.4948, -0.7221, -0.5183,\n",
       "         -0.7515,  0.5925, -0.7917, -0.7684, -0.8022,  0.6282, -0.5913, -0.7053,\n",
       "         -0.7850, -0.9125, -0.5494, -0.7391, -0.6494, -0.6341, -0.7141, -0.6461,\n",
       "          0.4461, -0.7290, -0.8905, -0.7300, -0.8014, -0.5017, -0.7073, -0.4693,\n",
       "         -0.8167, -0.8685, -0.6286, -0.3663, -0.4624, -0.6548, -0.5105, -0.8577,\n",
       "         -0.7082, -0.6414, -0.9604, -0.7584,  0.4060, -0.7011, -0.4215, -0.8319,\n",
       "         -0.6101, -0.6769, -0.7120, -0.7307, -0.6588, -0.6227,  0.4871, -0.6272,\n",
       "         -0.7118, -0.6470, -0.8334, -0.6752, -0.6249, -0.7917, -0.7455, -0.7075,\n",
       "         -0.6565,  0.4223, -0.7403, -0.7732,  0.4188, -0.6691, -0.4880, -0.8023,\n",
       "          0.5144, -0.7286, -0.7700, -0.1970, -0.7653, -0.6536, -0.7309, -0.4310,\n",
       "         -0.8147, -0.5560, -0.5598,  0.6860, -0.7262, -0.7932, -0.7009, -0.6544,\n",
       "         -0.5592, -0.5101, -0.8811, -0.6656,  0.3789, -0.8555, -0.7569, -0.5789,\n",
       "         -0.8761, -0.8504, -0.4819, -0.4685, -0.4201, -0.5366, -0.4998,  0.7500,\n",
       "         -0.8570, -0.6798, -0.7330,  0.3186, -0.8452, -0.6699, -0.7125, -0.8785,\n",
       "          0.4143, -0.8423, -0.8788,  0.4378, -0.7596, -0.7154, -0.6755, -0.7477,\n",
       "         -0.7023, -0.9016,  0.6877, -0.6909, -0.8704, -0.7621, -0.7638, -0.7218,\n",
       "         -0.8875, -0.8056, -0.8121, -0.8404, -0.5628, -0.7864, -0.7388, -0.4971,\n",
       "         -0.9078, -0.4976, -0.7602, -0.7440, -0.7474, -0.5443, -0.8375, -0.5272,\n",
       "         -0.6679, -0.7957, -0.7235, -0.7402,  0.4500, -0.7355, -0.7403, -0.6194,\n",
       "         -0.9309, -0.5824, -0.6811, -0.7649, -0.7824,  0.4590, -0.6091, -0.7732,\n",
       "         -0.8638, -0.6232, -0.3938, -0.5391, -0.8077, -0.8364, -0.7814, -0.7247,\n",
       "         -0.7690, -0.5512, -0.7908, -0.6590,  0.5388, -0.7267, -0.7061, -0.5368,\n",
       "         -0.7515, -0.6943, -0.7265, -0.7667, -0.8403, -0.7684,  0.4504, -0.6321,\n",
       "         -0.6086, -0.6516, -0.6027, -0.7116, -0.8341,  0.5283, -0.7961, -0.7427,\n",
       "         -0.6207, -0.8116, -0.7329,  0.7556, -0.8105, -0.8091, -0.8173, -0.5183,\n",
       "         -0.7764,  0.3530, -0.7249, -0.5669, -0.9048, -0.7848, -0.7442, -0.8188,\n",
       "         -0.9490, -0.5886, -0.8287, -0.7480, -0.7892, -0.5591, -0.6825, -0.7744,\n",
       "         -0.8613, -0.7335, -0.5600, -0.6062, -0.6173, -0.5522, -0.7321, -0.9299,\n",
       "         -0.8716, -0.7768, -0.5556, -0.7193, -0.6583,  0.4034,  0.3383,  0.4373,\n",
       "         -0.5104, -0.5666, -0.7151, -0.6840, -0.6976, -0.5371, -0.7784, -0.6152,\n",
       "         -0.7539, -0.7593, -0.8273, -0.8007, -0.6352, -0.6969, -0.7754, -0.6368,\n",
       "         -0.7697, -0.5234, -0.7042, -0.7823, -0.7485,  0.6154, -0.8414,  0.3288,\n",
       "         -0.7265, -0.7399,  0.3267, -0.6201, -0.7184, -0.6879, -0.7728, -0.6607,\n",
       "         -0.5918, -0.8979, -0.6981,  0.4384, -0.5948, -0.6703, -0.5958, -0.8184,\n",
       "         -0.6923, -0.7334, -0.8638,  0.5955, -0.7087, -0.5743, -0.6606,  0.4945,\n",
       "         -0.9303, -0.7032, -0.4639, -0.6650, -0.6262, -0.5904, -0.8311, -0.5393,\n",
       "         -0.8273, -0.5910, -0.5958, -0.7012, -0.8669, -0.6726, -0.6267, -0.7384,\n",
       "         -0.7006, -0.7546, -0.7159, -0.6319, -0.7780]),\n",
       " MultivariateNormal(loc: torch.Size([3986])),\n",
       " MultivariateNormal(loc: torch.Size([997])))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y, preds_train, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matern Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        # SKI requires a grid size hyperparameter. This util can help with that\n",
    "        grid_size = gpytorch.utils.grid.choose_grid_size(train_x)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "            gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(ard_num_dims=5),\n",
    "            ), grid_size=grid_size, num_dims=5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model_mat = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.854\n",
      "Iter 2/50 - Loss: 0.825\n",
      "Iter 3/50 - Loss: 0.798\n",
      "Iter 4/50 - Loss: 0.769\n",
      "Iter 5/50 - Loss: 0.742\n",
      "Iter 6/50 - Loss: 0.714\n",
      "Iter 7/50 - Loss: 0.688\n",
      "Iter 8/50 - Loss: 0.662\n",
      "Iter 9/50 - Loss: 0.637\n",
      "Iter 10/50 - Loss: 0.613\n",
      "Iter 11/50 - Loss: 0.591\n",
      "Iter 12/50 - Loss: 0.569\n",
      "Iter 13/50 - Loss: 0.547\n",
      "Iter 14/50 - Loss: 0.530\n",
      "Iter 15/50 - Loss: 0.516\n",
      "Iter 16/50 - Loss: 0.501\n",
      "Iter 17/50 - Loss: 0.488\n",
      "Iter 18/50 - Loss: 0.481\n",
      "Iter 19/50 - Loss: 0.473\n",
      "Iter 20/50 - Loss: 0.472\n",
      "Iter 21/50 - Loss: 0.466\n",
      "Iter 22/50 - Loss: 0.472\n",
      "Iter 23/50 - Loss: 0.469\n",
      "Iter 24/50 - Loss: 0.475\n",
      "Iter 25/50 - Loss: 0.475\n",
      "Iter 26/50 - Loss: 0.477\n",
      "Iter 27/50 - Loss: 0.484\n",
      "Iter 28/50 - Loss: 0.484\n",
      "Iter 29/50 - Loss: 0.482\n",
      "Iter 30/50 - Loss: 0.487\n",
      "Iter 31/50 - Loss: 0.492\n",
      "Iter 32/50 - Loss: 0.487\n",
      "Iter 33/50 - Loss: 0.480\n",
      "Iter 34/50 - Loss: 0.494\n",
      "Iter 35/50 - Loss: 0.492\n",
      "Iter 36/50 - Loss: 0.481\n",
      "Iter 37/50 - Loss: 0.492\n",
      "Iter 38/50 - Loss: 0.508\n",
      "Iter 39/50 - Loss: 0.496\n",
      "Iter 40/50 - Loss: 0.508\n",
      "Iter 41/50 - Loss: 0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:296: UserWarning: CG terminated in 1000 iterations with average residual norm 4.756214141845703 which is larger than the tolerance of 1 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 42/50 - Loss: 0.526\n",
      "Iter 43/50 - Loss: 0.522\n",
      "Iter 44/50 - Loss: 0.557\n",
      "Iter 45/50 - Loss: 0.574\n",
      "Iter 46/50 - Loss: 0.538\n",
      "Iter 47/50 - Loss: 0.573\n",
      "Iter 48/50 - Loss: 0.575\n",
      "Iter 49/50 - Loss: 0.613\n",
      "Iter 50/50 - Loss: 0.587\n",
      "CPU times: user 2h 28min 38s, sys: 1min 39s, total: 2h 30min 18s\n",
      "Wall time: 28min 36s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model_mat.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model_mat.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model_mat)\n",
    "\n",
    "def train():\n",
    "    training_iterations = 50\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model_mat(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/gpytorch/utils/linear_cg.py:296: UserWarning: CG terminated in 1000 iterations with average residual norm 1.2756239175796509 which is larger than the tolerance of 0.01 specified by gpytorch.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a gpytorch.settings.max_cg_iterations(value) context.\n",
      "  \" a gpytorch.settings.max_cg_iterations(value) context.\".format(k + 1, residual_norm.mean(), tolerance)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.3079177737236023\n",
      "Test MAE: 0.31456151604652405\n"
     ]
    }
   ],
   "source": [
    "model_mat.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds_train = model_mat(train_x)\n",
    "    \n",
    "print('Train MAE: {}'.format(torch.mean(torch.abs(preds_train.mean - train_y))))\n",
    "\n",
    "model_mat.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    preds = model_mat(test_x)\n",
    "    \n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
