{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Nanophotonics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomsx = torch.Tensor(loadmat('FOMs_x.mat', mat_dtype=True)['RVs_rnd'])\n",
    "fomsy = torch.Tensor(loadmat('FOMs_y.mat', mat_dtype=True)['FOMs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F0 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0 = torch.cat((fomsx[:,0:5], fomsy[:,0:1]), 1)\n",
    "f0 = f0[torch.randperm(f0.size()[0])]\n",
    "data = f0\n",
    "#if not os.path.isfile('elevators.mat'):\n",
    "#    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "#    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', 'elevators.mat')\n",
    "\n",
    "#data = torch.Tensor(loadmat('elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 316070.625\n",
      "Iter 2/60 - Loss: 309448.188\n",
      "Iter 3/60 - Loss: 308253.219\n",
      "Iter 4/60 - Loss: 303485.375\n",
      "Iter 5/60 - Loss: 299088.781\n",
      "Iter 6/60 - Loss: 295577.312\n",
      "Iter 7/60 - Loss: 292145.844\n",
      "Iter 8/60 - Loss: 288821.594\n",
      "Iter 9/60 - Loss: 286085.781\n",
      "Iter 10/60 - Loss: 283105.406\n",
      "Iter 11/60 - Loss: 279701.938\n",
      "Iter 12/60 - Loss: 276345.469\n",
      "Iter 13/60 - Loss: 273538.188\n",
      "Iter 14/60 - Loss: 270164.844\n",
      "Iter 15/60 - Loss: 266928.844\n",
      "Iter 16/60 - Loss: 263319.469\n",
      "Iter 17/60 - Loss: 260412.109\n",
      "Iter 18/60 - Loss: 257992.734\n",
      "Iter 19/60 - Loss: 254647.266\n",
      "Iter 20/60 - Loss: 251479.234\n",
      "Iter 21/60 - Loss: 248186.828\n",
      "Iter 22/60 - Loss: 247364.766\n",
      "Iter 23/60 - Loss: 250328.406\n",
      "Iter 24/60 - Loss: 248557.516\n",
      "Iter 25/60 - Loss: 245838.516\n",
      "Iter 26/60 - Loss: 242942.141\n",
      "Iter 27/60 - Loss: 238439.234\n",
      "Iter 28/60 - Loss: 235690.781\n",
      "Iter 29/60 - Loss: 232020.172\n",
      "Iter 30/60 - Loss: 228226.219\n",
      "Iter 31/60 - Loss: 225700.219\n",
      "Iter 32/60 - Loss: 222669.406\n",
      "Iter 33/60 - Loss: 220501.422\n",
      "Iter 34/60 - Loss: 218239.938\n",
      "Iter 35/60 - Loss: 215166.641\n",
      "Iter 36/60 - Loss: 214831.391\n",
      "Iter 37/60 - Loss: 210784.156\n",
      "Iter 38/60 - Loss: 206637.703\n",
      "Iter 39/60 - Loss: 205103.094\n",
      "Iter 40/60 - Loss: 204473.000\n",
      "Iter 41/60 - Loss: 198610.922\n",
      "Iter 42/60 - Loss: 197747.938\n",
      "Iter 43/60 - Loss: 201636.812\n",
      "Iter 44/60 - Loss: 200885.438\n",
      "Iter 45/60 - Loss: 203277.422\n",
      "Iter 46/60 - Loss: 195368.531\n",
      "Iter 47/60 - Loss: 196329.344\n",
      "Iter 48/60 - Loss: 191140.328\n",
      "Iter 49/60 - Loss: 189039.906\n",
      "Iter 50/60 - Loss: 185390.203\n",
      "Iter 51/60 - Loss: 185105.984\n",
      "Iter 52/60 - Loss: 187292.297\n",
      "Iter 53/60 - Loss: 179829.859\n",
      "Iter 54/60 - Loss: 178866.828\n",
      "Iter 55/60 - Loss: 174057.438\n",
      "Iter 56/60 - Loss: 170771.594\n",
      "Iter 57/60 - Loss: 170368.859\n",
      "Iter 58/60 - Loss: 168962.688\n",
      "Iter 59/60 - Loss: 162979.641\n",
      "Iter 60/60 - Loss: 167712.719\n",
      "CPU times: user 10min 6s, sys: 4.97 s, total: 10min 11s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 426.04217529296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:247: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    train_preds = model(train_x)\n",
    "print('Train MAE: {}'.format(torch.mean(torch.abs(train_preds.mean - train_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 534.7948608398438\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = torch.cat((fomsx[:,0:5], fomsy[:,1:2]), 1)\n",
    "f1 = f1[torch.randperm(f0.size()[0])]\n",
    "data = f1\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 78329.734\n",
      "Iter 2/60 - Loss: 75709.570\n",
      "Iter 3/60 - Loss: 71425.562\n",
      "Iter 4/60 - Loss: 69685.180\n",
      "Iter 5/60 - Loss: 67653.703\n",
      "Iter 6/60 - Loss: 66153.445\n",
      "Iter 7/60 - Loss: 64489.352\n",
      "Iter 8/60 - Loss: 62887.633\n",
      "Iter 9/60 - Loss: 60827.512\n",
      "Iter 10/60 - Loss: 59943.648\n",
      "Iter 11/60 - Loss: 58588.652\n",
      "Iter 12/60 - Loss: 57837.777\n",
      "Iter 13/60 - Loss: 55990.848\n",
      "Iter 14/60 - Loss: 55376.008\n",
      "Iter 15/60 - Loss: 54486.082\n",
      "Iter 16/60 - Loss: 52747.555\n",
      "Iter 17/60 - Loss: 51616.535\n",
      "Iter 18/60 - Loss: 50745.480\n",
      "Iter 19/60 - Loss: 49326.977\n",
      "Iter 20/60 - Loss: 48739.941\n",
      "Iter 21/60 - Loss: 47984.977\n",
      "Iter 22/60 - Loss: 46625.273\n",
      "Iter 23/60 - Loss: 45830.047\n",
      "Iter 24/60 - Loss: 45367.789\n",
      "Iter 25/60 - Loss: 45465.441\n",
      "Iter 26/60 - Loss: 43923.406\n",
      "Iter 27/60 - Loss: 43364.781\n",
      "Iter 28/60 - Loss: 42830.148\n",
      "Iter 29/60 - Loss: 41213.836\n",
      "Iter 30/60 - Loss: 40647.664\n",
      "Iter 31/60 - Loss: 40651.199\n",
      "Iter 32/60 - Loss: 40183.098\n",
      "Iter 33/60 - Loss: 40771.688\n",
      "Iter 34/60 - Loss: 39745.715\n",
      "Iter 35/60 - Loss: 39858.695\n",
      "Iter 36/60 - Loss: 39145.352\n",
      "Iter 37/60 - Loss: 37649.496\n",
      "Iter 38/60 - Loss: 37552.227\n",
      "Iter 39/60 - Loss: 36788.664\n",
      "Iter 40/60 - Loss: 35911.059\n",
      "Iter 41/60 - Loss: 35208.230\n",
      "Iter 42/60 - Loss: 36342.887\n",
      "Iter 43/60 - Loss: 34553.723\n",
      "Iter 44/60 - Loss: 35913.188\n",
      "Iter 45/60 - Loss: 34607.828\n",
      "Iter 46/60 - Loss: 33984.496\n",
      "Iter 47/60 - Loss: 34229.812\n",
      "Iter 48/60 - Loss: 32320.033\n",
      "Iter 49/60 - Loss: 31900.975\n",
      "Iter 50/60 - Loss: 30898.695\n",
      "Iter 51/60 - Loss: 30991.502\n",
      "Iter 52/60 - Loss: 30384.252\n",
      "Iter 53/60 - Loss: 33972.742\n",
      "Iter 54/60 - Loss: 32622.428\n",
      "Iter 55/60 - Loss: 31640.092\n",
      "Iter 56/60 - Loss: 31429.320\n",
      "Iter 57/60 - Loss: 30506.848\n",
      "Iter 58/60 - Loss: 30729.477\n",
      "Iter 59/60 - Loss: 28880.879\n",
      "Iter 60/60 - Loss: 29676.658\n",
      "CPU times: user 10min 8s, sys: 5.74 s, total: 10min 14s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 151.2612762451172\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4983, 6])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = torch.cat((fomsx[:,0:5], fomsy[:,2:3]), 1)\n",
    "f2 = f2[torch.randperm(f0.size()[0])]\n",
    "data = f2\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 140728.109\n",
      "Iter 2/60 - Loss: 135678.375\n",
      "Iter 3/60 - Loss: 128216.062\n",
      "Iter 4/60 - Loss: 125743.219\n",
      "Iter 5/60 - Loss: 120583.156\n",
      "Iter 6/60 - Loss: 118685.500\n",
      "Iter 7/60 - Loss: 115023.516\n",
      "Iter 8/60 - Loss: 111105.977\n",
      "Iter 9/60 - Loss: 107162.234\n",
      "Iter 10/60 - Loss: 103565.539\n",
      "Iter 11/60 - Loss: 100971.406\n",
      "Iter 12/60 - Loss: 99462.922\n",
      "Iter 13/60 - Loss: 97114.367\n",
      "Iter 14/60 - Loss: 94522.398\n",
      "Iter 15/60 - Loss: 92956.523\n",
      "Iter 16/60 - Loss: 91233.789\n",
      "Iter 17/60 - Loss: 88963.742\n",
      "Iter 18/60 - Loss: 87190.609\n",
      "Iter 19/60 - Loss: 85849.875\n",
      "Iter 20/60 - Loss: 84115.234\n",
      "Iter 21/60 - Loss: 82339.727\n",
      "Iter 22/60 - Loss: 81023.695\n",
      "Iter 23/60 - Loss: 79612.508\n",
      "Iter 24/60 - Loss: 78161.945\n",
      "Iter 25/60 - Loss: 76797.984\n",
      "Iter 26/60 - Loss: 75512.547\n",
      "Iter 27/60 - Loss: 73915.328\n",
      "Iter 28/60 - Loss: 72722.336\n",
      "Iter 29/60 - Loss: 72049.648\n",
      "Iter 30/60 - Loss: 72479.477\n",
      "Iter 31/60 - Loss: 72830.672\n",
      "Iter 32/60 - Loss: 72102.078\n",
      "Iter 33/60 - Loss: 70202.242\n",
      "Iter 34/60 - Loss: 68431.594\n",
      "Iter 35/60 - Loss: 67884.445\n",
      "Iter 36/60 - Loss: 66686.789\n",
      "Iter 37/60 - Loss: 65366.188\n",
      "Iter 38/60 - Loss: 65192.914\n",
      "Iter 39/60 - Loss: 63029.398\n",
      "Iter 40/60 - Loss: 62781.074\n",
      "Iter 41/60 - Loss: 60844.609\n",
      "Iter 42/60 - Loss: 59856.879\n",
      "Iter 43/60 - Loss: 58196.293\n",
      "Iter 44/60 - Loss: 57167.395\n",
      "Iter 45/60 - Loss: 57629.391\n",
      "Iter 46/60 - Loss: 58846.375\n",
      "Iter 47/60 - Loss: 60364.836\n",
      "Iter 48/60 - Loss: 59726.180\n",
      "Iter 49/60 - Loss: 65437.609\n",
      "Iter 50/60 - Loss: 60426.199\n",
      "Iter 51/60 - Loss: 58462.289\n",
      "Iter 52/60 - Loss: 59335.609\n",
      "Iter 53/60 - Loss: 57026.777\n",
      "Iter 54/60 - Loss: 56801.020\n",
      "Iter 55/60 - Loss: 53557.344\n",
      "Iter 56/60 - Loss: 54993.430\n",
      "Iter 57/60 - Loss: 51756.773\n",
      "Iter 58/60 - Loss: 51647.512\n",
      "Iter 59/60 - Loss: 49915.496\n",
      "Iter 60/60 - Loss: 49924.500\n",
      "CPU times: user 10min 10s, sys: 6.1 s, total: 10min 16s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 223.6408233642578\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with regular GP with squared exponential kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = torch.cat((fomsx[:,0:5], fomsy[:,2:3]), 1)\n",
    "f2 = f2[torch.randperm(f0.size()[0])]\n",
    "data = f2\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4983, 5]),\n",
       " torch.Size([4983]),\n",
       " torch.Size([3986, 5]),\n",
       " torch.Size([3986]),\n",
       " torch.Size([997, 5]),\n",
       " torch.Size([997]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size(), y.size(), train_x.size(), train_y.size(), test_x.size(), test_y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 96036.086   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 87546.477   lengthscale: 0.644   noise: 0.744\n",
      "Iter 3/50 - Loss: 79903.406   lengthscale: 0.598   noise: 0.798\n",
      "Iter 4/50 - Loss: 73060.273   lengthscale: 0.555   noise: 0.853\n",
      "Iter 5/50 - Loss: 66970.258   lengthscale: 0.514   noise: 0.910\n",
      "Iter 6/50 - Loss: 61604.480   lengthscale: 0.476   noise: 0.969\n",
      "Iter 7/50 - Loss: 56951.957   lengthscale: 0.440   noise: 1.028\n",
      "Iter 8/50 - Loss: 53007.867   lengthscale: 0.408   noise: 1.087\n",
      "Iter 9/50 - Loss: 49765.117   lengthscale: 0.379   noise: 1.147\n",
      "Iter 10/50 - Loss: 47176.375   lengthscale: 0.354   noise: 1.206\n",
      "Iter 11/50 - Loss: 45123.332   lengthscale: 0.334   noise: 1.265\n",
      "Iter 12/50 - Loss: 43401.844   lengthscale: 0.319   noise: 1.323\n",
      "Iter 13/50 - Loss: 41793.348   lengthscale: 0.309   noise: 1.379\n",
      "Iter 14/50 - Loss: 40157.277   lengthscale: 0.304   noise: 1.435\n",
      "Iter 15/50 - Loss: 38466.930   lengthscale: 0.303   noise: 1.489\n",
      "Iter 16/50 - Loss: 36773.918   lengthscale: 0.306   noise: 1.542\n",
      "Iter 17/50 - Loss: 35149.277   lengthscale: 0.311   noise: 1.593\n",
      "Iter 18/50 - Loss: 33655.629   lengthscale: 0.318   noise: 1.643\n",
      "Iter 19/50 - Loss: 32322.305   lengthscale: 0.326   noise: 1.693\n",
      "Iter 20/50 - Loss: 31158.732   lengthscale: 0.334   noise: 1.741\n",
      "Iter 21/50 - Loss: 30147.123   lengthscale: 0.343   noise: 1.787\n",
      "Iter 22/50 - Loss: 29267.129   lengthscale: 0.351   noise: 1.833\n",
      "Iter 23/50 - Loss: 28485.979   lengthscale: 0.358   noise: 1.878\n",
      "Iter 24/50 - Loss: 27780.295   lengthscale: 0.364   noise: 1.922\n",
      "Iter 25/50 - Loss: 27133.119   lengthscale: 0.369   noise: 1.966\n",
      "Iter 26/50 - Loss: 26525.271   lengthscale: 0.372   noise: 2.008\n",
      "Iter 27/50 - Loss: 25947.887   lengthscale: 0.374   noise: 2.050\n",
      "Iter 28/50 - Loss: 25391.916   lengthscale: 0.375   noise: 2.091\n",
      "Iter 29/50 - Loss: 24856.092   lengthscale: 0.374   noise: 2.131\n",
      "Iter 30/50 - Loss: 24341.723   lengthscale: 0.372   noise: 2.171\n",
      "Iter 31/50 - Loss: 23847.844   lengthscale: 0.369   noise: 2.209\n",
      "Iter 32/50 - Loss: 23373.531   lengthscale: 0.366   noise: 2.247\n",
      "Iter 33/50 - Loss: 22922.652   lengthscale: 0.361   noise: 2.285\n",
      "Iter 34/50 - Loss: 22497.012   lengthscale: 0.356   noise: 2.321\n",
      "Iter 35/50 - Loss: 22095.977   lengthscale: 0.351   noise: 2.357\n",
      "Iter 36/50 - Loss: 21723.783   lengthscale: 0.346   noise: 2.391\n",
      "Iter 37/50 - Loss: 21376.152   lengthscale: 0.341   noise: 2.425\n",
      "Iter 38/50 - Loss: 21055.582   lengthscale: 0.336   noise: 2.459\n",
      "Iter 39/50 - Loss: 20757.887   lengthscale: 0.332   noise: 2.491\n",
      "Iter 40/50 - Loss: 20480.650   lengthscale: 0.328   noise: 2.523\n",
      "Iter 41/50 - Loss: 20220.244   lengthscale: 0.325   noise: 2.553\n",
      "Iter 42/50 - Loss: 19971.057   lengthscale: 0.322   noise: 2.584\n",
      "Iter 43/50 - Loss: 19733.096   lengthscale: 0.320   noise: 2.613\n",
      "Iter 44/50 - Loss: 19502.645   lengthscale: 0.319   noise: 2.641\n",
      "Iter 45/50 - Loss: 19278.297   lengthscale: 0.318   noise: 2.669\n",
      "Iter 46/50 - Loss: 19058.918   lengthscale: 0.318   noise: 2.697\n",
      "Iter 47/50 - Loss: 18844.143   lengthscale: 0.319   noise: 2.723\n",
      "Iter 48/50 - Loss: 18637.434   lengthscale: 0.320   noise: 2.749\n",
      "Iter 49/50 - Loss: 18436.332   lengthscale: 0.321   noise: 2.775\n",
      "Iter 50/50 - Loss: 18244.043   lengthscale: 0.323   noise: 2.800\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 5 and 1 in dimension 1 at /Users/distiller/project/conda/conda-bld/pytorch_1570710797334/work/aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-6cecd7e27428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_pred_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mobserved_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/ML/lib/python3.7/site-packages/gpytorch/models/exact_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0mtrain_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                 \u001b[0mfull_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;31m# Get the joint distribution for training/test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 5 and 1 in dimension 1 at /Users/distiller/project/conda/conda-bld/pytorch_1570710797334/work/aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_x = torch.linspace(0, 1, 51)\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'observed_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-d071b2d9b15e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Get upper and lower confidence bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobserved_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfidence_region\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Plot training data as black stars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'observed_pred' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Initialize plot\n",
    "    f, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.numpy(), train_y.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
