{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensors for all fidelities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "fomsx = torch.Tensor(loadmat('FOMs_x.mat', mat_dtype=True)['RVs_rnd'])\n",
    "fomsy = torch.Tensor(loadmat('FOMs_y.mat', mat_dtype=True)['FOMs'])\n",
    "f0 = torch.cat((fomsx[:,0:5], fomsy[:,0:1]), 1)\n",
    "f0 = f0[torch.randperm(f0.size()[0])]\n",
    "f1 = torch.cat((fomsx[:,0:5], fomsy[:,1:2]), 1)\n",
    "f1 = f1[torch.randperm(f0.size()[0])]\n",
    "f2 = torch.cat((fomsx[:,0:5], fomsy[:,2:3]), 1)\n",
    "f2 = f2[torch.randperm(f0.size()[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F0 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f0\n",
    "#if not os.path.isfile('elevators.mat'):\n",
    "#    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "#    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', 'elevators.mat')\n",
    "\n",
    "#data = torch.Tensor(loadmat('elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "class LargeFeatureExtractor(torch.nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super(LargeFeatureExtractor, self).__init__()\n",
    "        self.add_module('linear1', torch.nn.Linear(data_dim, 1000))\n",
    "        self.add_module('relu1', torch.nn.ReLU())\n",
    "        self.add_module('linear2', torch.nn.Linear(1000, 500))\n",
    "        self.add_module('relu2', torch.nn.ReLU())\n",
    "        self.add_module('linear3', torch.nn.Linear(500, 50))\n",
    "        self.add_module('relu3', torch.nn.ReLU())\n",
    "        self.add_module('linear4', torch.nn.Linear(50, 2))\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n",
    "                gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=2)),\n",
    "                num_dims=2, grid_size=100\n",
    "            )\n",
    "            self.feature_extractor = feature_extractor\n",
    "\n",
    "        def forward(self, x):\n",
    "            # We're first putting our data through a deep net (feature extractor)\n",
    "            # We're also scaling the features so that they're nice values\n",
    "            projected_x = self.feature_extractor(x)\n",
    "            projected_x = projected_x - projected_x.min(0)[0]\n",
    "            projected_x = 2 * (projected_x / projected_x.max(0)[0]) - 1\n",
    "\n",
    "            mean_x = self.mean_module(projected_x)\n",
    "            covar_x = self.covar_module(projected_x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 316070.625\n",
      "Iter 2/60 - Loss: 309448.188\n",
      "Iter 3/60 - Loss: 308253.219\n",
      "Iter 4/60 - Loss: 303485.375\n",
      "Iter 5/60 - Loss: 299088.781\n",
      "Iter 6/60 - Loss: 295577.312\n",
      "Iter 7/60 - Loss: 292145.844\n",
      "Iter 8/60 - Loss: 288821.594\n",
      "Iter 9/60 - Loss: 286085.781\n",
      "Iter 10/60 - Loss: 283105.406\n",
      "Iter 11/60 - Loss: 279701.938\n",
      "Iter 12/60 - Loss: 276345.469\n",
      "Iter 13/60 - Loss: 273538.188\n",
      "Iter 14/60 - Loss: 270164.844\n",
      "Iter 15/60 - Loss: 266928.844\n",
      "Iter 16/60 - Loss: 263319.469\n",
      "Iter 17/60 - Loss: 260412.109\n",
      "Iter 18/60 - Loss: 257992.734\n",
      "Iter 19/60 - Loss: 254647.266\n",
      "Iter 20/60 - Loss: 251479.234\n",
      "Iter 21/60 - Loss: 248186.828\n",
      "Iter 22/60 - Loss: 247364.766\n",
      "Iter 23/60 - Loss: 250328.406\n",
      "Iter 24/60 - Loss: 248557.516\n",
      "Iter 25/60 - Loss: 245838.516\n",
      "Iter 26/60 - Loss: 242942.141\n",
      "Iter 27/60 - Loss: 238439.234\n",
      "Iter 28/60 - Loss: 235690.781\n",
      "Iter 29/60 - Loss: 232020.172\n",
      "Iter 30/60 - Loss: 228226.219\n",
      "Iter 31/60 - Loss: 225700.219\n",
      "Iter 32/60 - Loss: 222669.406\n",
      "Iter 33/60 - Loss: 220501.422\n",
      "Iter 34/60 - Loss: 218239.938\n",
      "Iter 35/60 - Loss: 215166.641\n",
      "Iter 36/60 - Loss: 214831.391\n",
      "Iter 37/60 - Loss: 210784.156\n",
      "Iter 38/60 - Loss: 206637.703\n",
      "Iter 39/60 - Loss: 205103.094\n",
      "Iter 40/60 - Loss: 204473.000\n",
      "Iter 41/60 - Loss: 198610.922\n",
      "Iter 42/60 - Loss: 197747.938\n",
      "Iter 43/60 - Loss: 201636.812\n",
      "Iter 44/60 - Loss: 200885.438\n",
      "Iter 45/60 - Loss: 203277.422\n",
      "Iter 46/60 - Loss: 195368.531\n",
      "Iter 47/60 - Loss: 196329.344\n",
      "Iter 48/60 - Loss: 191140.328\n",
      "Iter 49/60 - Loss: 189039.906\n",
      "Iter 50/60 - Loss: 185390.203\n",
      "Iter 51/60 - Loss: 185105.984\n",
      "Iter 52/60 - Loss: 187292.297\n",
      "Iter 53/60 - Loss: 179829.859\n",
      "Iter 54/60 - Loss: 178866.828\n",
      "Iter 55/60 - Loss: 174057.438\n",
      "Iter 56/60 - Loss: 170771.594\n",
      "Iter 57/60 - Loss: 170368.859\n",
      "Iter 58/60 - Loss: 168962.688\n",
      "Iter 59/60 - Loss: 162979.641\n",
      "Iter 60/60 - Loss: 167712.719\n",
      "CPU times: user 10min 6s, sys: 4.97 s, total: 10min 11s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 426.04217529296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.7/site-packages/gpytorch/models/exact_gp.py:247: UserWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  \"The input matches the stored training data. Did you forget to call model.train()?\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    train_preds = model(train_x)\n",
    "print('Train MAE: {}'.format(torch.mean(torch.abs(train_preds.mean - train_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 534.7948608398438\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f1\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 78329.734\n",
      "Iter 2/60 - Loss: 75709.570\n",
      "Iter 3/60 - Loss: 71425.562\n",
      "Iter 4/60 - Loss: 69685.180\n",
      "Iter 5/60 - Loss: 67653.703\n",
      "Iter 6/60 - Loss: 66153.445\n",
      "Iter 7/60 - Loss: 64489.352\n",
      "Iter 8/60 - Loss: 62887.633\n",
      "Iter 9/60 - Loss: 60827.512\n",
      "Iter 10/60 - Loss: 59943.648\n",
      "Iter 11/60 - Loss: 58588.652\n",
      "Iter 12/60 - Loss: 57837.777\n",
      "Iter 13/60 - Loss: 55990.848\n",
      "Iter 14/60 - Loss: 55376.008\n",
      "Iter 15/60 - Loss: 54486.082\n",
      "Iter 16/60 - Loss: 52747.555\n",
      "Iter 17/60 - Loss: 51616.535\n",
      "Iter 18/60 - Loss: 50745.480\n",
      "Iter 19/60 - Loss: 49326.977\n",
      "Iter 20/60 - Loss: 48739.941\n",
      "Iter 21/60 - Loss: 47984.977\n",
      "Iter 22/60 - Loss: 46625.273\n",
      "Iter 23/60 - Loss: 45830.047\n",
      "Iter 24/60 - Loss: 45367.789\n",
      "Iter 25/60 - Loss: 45465.441\n",
      "Iter 26/60 - Loss: 43923.406\n",
      "Iter 27/60 - Loss: 43364.781\n",
      "Iter 28/60 - Loss: 42830.148\n",
      "Iter 29/60 - Loss: 41213.836\n",
      "Iter 30/60 - Loss: 40647.664\n",
      "Iter 31/60 - Loss: 40651.199\n",
      "Iter 32/60 - Loss: 40183.098\n",
      "Iter 33/60 - Loss: 40771.688\n",
      "Iter 34/60 - Loss: 39745.715\n",
      "Iter 35/60 - Loss: 39858.695\n",
      "Iter 36/60 - Loss: 39145.352\n",
      "Iter 37/60 - Loss: 37649.496\n",
      "Iter 38/60 - Loss: 37552.227\n",
      "Iter 39/60 - Loss: 36788.664\n",
      "Iter 40/60 - Loss: 35911.059\n",
      "Iter 41/60 - Loss: 35208.230\n",
      "Iter 42/60 - Loss: 36342.887\n",
      "Iter 43/60 - Loss: 34553.723\n",
      "Iter 44/60 - Loss: 35913.188\n",
      "Iter 45/60 - Loss: 34607.828\n",
      "Iter 46/60 - Loss: 33984.496\n",
      "Iter 47/60 - Loss: 34229.812\n",
      "Iter 48/60 - Loss: 32320.033\n",
      "Iter 49/60 - Loss: 31900.975\n",
      "Iter 50/60 - Loss: 30898.695\n",
      "Iter 51/60 - Loss: 30991.502\n",
      "Iter 52/60 - Loss: 30384.252\n",
      "Iter 53/60 - Loss: 33972.742\n",
      "Iter 54/60 - Loss: 32622.428\n",
      "Iter 55/60 - Loss: 31640.092\n",
      "Iter 56/60 - Loss: 31429.320\n",
      "Iter 57/60 - Loss: 30506.848\n",
      "Iter 58/60 - Loss: 30729.477\n",
      "Iter 59/60 - Loss: 28880.879\n",
      "Iter 60/60 - Loss: 29676.658\n",
      "CPU times: user 10min 8s, sys: 5.74 s, total: 10min 14s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 151.2612762451172\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F2 Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f2\n",
    "\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "# Use the first 80% of the data for training, and the last 20% for testing.\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = train_x.size(-1)\n",
    "\n",
    "feature_extractor = LargeFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/60 - Loss: 140728.109\n",
      "Iter 2/60 - Loss: 135678.375\n",
      "Iter 3/60 - Loss: 128216.062\n",
      "Iter 4/60 - Loss: 125743.219\n",
      "Iter 5/60 - Loss: 120583.156\n",
      "Iter 6/60 - Loss: 118685.500\n",
      "Iter 7/60 - Loss: 115023.516\n",
      "Iter 8/60 - Loss: 111105.977\n",
      "Iter 9/60 - Loss: 107162.234\n",
      "Iter 10/60 - Loss: 103565.539\n",
      "Iter 11/60 - Loss: 100971.406\n",
      "Iter 12/60 - Loss: 99462.922\n",
      "Iter 13/60 - Loss: 97114.367\n",
      "Iter 14/60 - Loss: 94522.398\n",
      "Iter 15/60 - Loss: 92956.523\n",
      "Iter 16/60 - Loss: 91233.789\n",
      "Iter 17/60 - Loss: 88963.742\n",
      "Iter 18/60 - Loss: 87190.609\n",
      "Iter 19/60 - Loss: 85849.875\n",
      "Iter 20/60 - Loss: 84115.234\n",
      "Iter 21/60 - Loss: 82339.727\n",
      "Iter 22/60 - Loss: 81023.695\n",
      "Iter 23/60 - Loss: 79612.508\n",
      "Iter 24/60 - Loss: 78161.945\n",
      "Iter 25/60 - Loss: 76797.984\n",
      "Iter 26/60 - Loss: 75512.547\n",
      "Iter 27/60 - Loss: 73915.328\n",
      "Iter 28/60 - Loss: 72722.336\n",
      "Iter 29/60 - Loss: 72049.648\n",
      "Iter 30/60 - Loss: 72479.477\n",
      "Iter 31/60 - Loss: 72830.672\n",
      "Iter 32/60 - Loss: 72102.078\n",
      "Iter 33/60 - Loss: 70202.242\n",
      "Iter 34/60 - Loss: 68431.594\n",
      "Iter 35/60 - Loss: 67884.445\n",
      "Iter 36/60 - Loss: 66686.789\n",
      "Iter 37/60 - Loss: 65366.188\n",
      "Iter 38/60 - Loss: 65192.914\n",
      "Iter 39/60 - Loss: 63029.398\n",
      "Iter 40/60 - Loss: 62781.074\n",
      "Iter 41/60 - Loss: 60844.609\n",
      "Iter 42/60 - Loss: 59856.879\n",
      "Iter 43/60 - Loss: 58196.293\n",
      "Iter 44/60 - Loss: 57167.395\n",
      "Iter 45/60 - Loss: 57629.391\n",
      "Iter 46/60 - Loss: 58846.375\n",
      "Iter 47/60 - Loss: 60364.836\n",
      "Iter 48/60 - Loss: 59726.180\n",
      "Iter 49/60 - Loss: 65437.609\n",
      "Iter 50/60 - Loss: 60426.199\n",
      "Iter 51/60 - Loss: 58462.289\n",
      "Iter 52/60 - Loss: 59335.609\n",
      "Iter 53/60 - Loss: 57026.777\n",
      "Iter 54/60 - Loss: 56801.020\n",
      "Iter 55/60 - Loss: 53557.344\n",
      "Iter 56/60 - Loss: 54993.430\n",
      "Iter 57/60 - Loss: 51756.773\n",
      "Iter 58/60 - Loss: 51647.512\n",
      "Iter 59/60 - Loss: 49915.496\n",
      "Iter 60/60 - Loss: 49924.500\n",
      "CPU times: user 10min 10s, sys: 6.1 s, total: 10min 16s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.feature_extractor.parameters()},\n",
    "    {'params': model.covar_module.parameters()},\n",
    "    {'params': model.mean_module.parameters()},\n",
    "    {'params': model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 60\n",
    "def train():\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "with gpytorch.settings.use_toeplitz(True):\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad(), gpytorch.settings.use_toeplitz(False), gpytorch.settings.fast_pred_var():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 223.6408233642578\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
